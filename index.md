李剑
=====================
### 基本信息                
* 出生日期：1994/11/14                  
* 籍贯：江苏 盐城                
* 政治面貌：党员     
* 手机：147 5160 0355
* Email：lijiannuist@gmail.com 
* 地址：江苏省南京市孝陵卫街200号 
* 邮编：210094
* 主页  [Github.Homepage](https://github.com/lijiannuist)  [微博](https://weibo.com/3012693523/profile?rightmod=1&wvr=6&mod=personinfo&is_all=1)
* 2016.09- 南京理工大学  计算机学院 导师：[杨健](https://baike.baidu.com/item/%E6%9D%A8%E5%81%A5/9376288?fr=aladdin), 钱建军 
* 研究方向：目标检测，深度学习，计算机视觉，自动驾驶
* 2012.09-2016.06 南京信息工程大学  数学与统计学院  信息与计算科学 GPA: 88.35 TOP：4/71    
* 专业课程：数学分析、高等代数、概率论与数理统计、数据结构、数值分析

### 专业技能
* 	熟悉C/C++、Matlab、python、Jave、Mysql、Opencv、caffe、vtk、pytorch、ros、pandas。
* 	有较好的程序算法设计能力，较好的数学建模能力。
* 	英语CET-6（527），能进行日常简单交流，可熟练阅读英文资料。

###  实习&项目

1. 	**` `2017.10-2017.12` `多标签图像分类` `腾讯` `微信` `模式识别中心` `mentor：李岩  **

     此项目是[腾讯犀牛鸟精英研究生计划](http://cs.njust.edu.cn/43/55/c1817a148309/page.htm)，期间主要研究如何对微信公众号以及朋友圈图像打上多个标签。
 
1. 	**` `2017.07-2017.09` `车辆轨迹预测` `图森 ` `mentor：王乃岩**

     主要任务是对行车周边车辆进行轨迹预测，以此来避免碰撞。主要使用了kalman filter , xgboost , lstm 等技术对下一个时刻的车辆位置进行估计。同时建立预测的评价指标，评估预测结果。
     
1. 	**` `2016.1-2016.6` `中文手写字体检测与识别` `北京大学` `计算机研究所` `字形计算实验室` `mentor：连宙辉**

     本人于 2015.7 月参加北京大学计算所夏令营，通过北大推免预选拔。 9月本校推免失败，故于2016.1月起在北大计算所实习。实习内容涉及手写汉字二值化、分割、以及 OCR，使用 opencv、深度学习框架 caffe 开发，基于CASIA库训练的alexnet_model可参考：[HCCR](http://pan.baidu.com/s/1qYCbfqs)。

2. 	**` `2014-2015` ` 医学图像分割` `南京信息工程大学` `数学统计学院` `图像处理实验室`  `mentor：陈允杰**

     该项目是本人在数统院图像处理实验室完成。主要研究了基于统计的图像分割算法（K-means、FCM、GMM），并提出将非局部信息融入现有的模型当中，实验准确率较高，在导师的指导下文章已经写完，将投SCI期刊。同时，独立开发一套基于vtk的脑核磁共振图像三维分割系统。

3. 	**` `2013-2015` `自动指纹识别` `南京信息工程大学` `计算机学院` `指纹组` `mentor：梅园**

     该指纹组由四位本科生组成，由计算机学院老师带队，本人负责指纹方向场的建模和计算（PDE、DCT等），提出一种基于改进的偏微分模型的指纹方向场计算方法并申请获得专利，同时对现有的算法进行改进，提出一种谱方法计算指纹方向场，实验还在进行当中。除此以外，也曾研究以指纹方向场作为指纹特征对其进行分类。指纹组也开发出一套基于C++的自动指纹识别系统。在该系统中，本人负责实现指纹方向场的代码。

### 竞赛经历
1. 	**` `2017.09` `[Didi-Udacity无人驾驶挑战赛](http://research.xiaojukeji.com/)` `冠军` `奖金10万美金 **

     本次大赛赛题是无人驾驶，要求参赛者找到通过摄像头和 LIDAR 数据检测道路上的障碍物的最好方法。要求参赛者实时处理 LIDAR、RADAR及摄像头原始数据，输出障碍物位置、移除噪音和环境错误检测。最终的代码方案需要基于ROS 架构（Ubuntu 14.04, ROS  Indigo）、并能够在搭载 I7 和 Titan  X的 平台上以至少 10Hz 运行。我们提出了一种基于多传感器由粗到细的障碍物检测框架（multi-sensor coarse-to-fine detection  framework）。在该框架中，对于相机图像，采用深度学习算法（YOLO），对于点云数据，采用精心设计的聚类和识别算法。该框架能在达到最高的检测指标（IOU）的同时也能够有很快的检测速度，最终我们的成绩在两轮leadboard上都排名第一，现场真车测试速度达到20FPS。[学院新闻报道](http://cs.njust.edu.cn/49/27/c1817a149799/page.htm) , [金陵晚报](http://jlwb.njnews.cn/html/2017-09/22/content_110945.htm), [知乎专栏](https://zhuanlan.zhihu.com/p/29907537)

2. 	**` `2017.01` `[Ucar-Bitiger深度学习无人驾驶挑战赛](https://www.bittiger.io/competition?utm_source=Zhihu&utm_medium=Lurenjia&utm_content=post733)` `冠军` `奖金5000美金 **

     本次大赛的任务是利用神州专车北美实验室提供的10000张标注好的训练图像数据，运用深度学习模型检测出给定图像中的车辆、行人、交通标志等物体的Bounding Box并识别。本次大赛包含两个赛季，其中初赛和复赛需要选手线上提交2000张和3000张图像的检测结果。 我们通过改进R-FCN 以及集成更多的特征提取网络来提升我们的检测性能，最终获得冠军。[学院新闻报道](http://cs.njust.edu.cn/1b/66/c1817a138086/page.htm)

3. 	**` `2016.11` `上海BOT大数据竞赛视觉组` `亚军` `奖金5万人民币 **

     本次该比赛分三个赛季，包含三个任务，分别为：动物类别识别，货架商品检测，视觉问答。在货架商品检测任务中，我们引入反卷积修改SSD 网络以增加图像分辨率，同时由于图像标志较少，只有几百张，我们对图像进行裁剪拼接以增广数据. [学院新闻报道](http://cs.njust.edu.cn/08/81/c1817a133249/page.htm)

### 个人荣誉
*    2017.11`  `南京理工大学校长奖章；
*    2017.10`  `研究生国家奖学金，校一等奖学金；
*    2016.09`  `全国研究生数学建模国家二等奖；
* 	2015.03`  `美国大学数学建模国家二等奖；
* 	2014.11`  `“高教社杯”全国大学生数学建模大赛国家二等奖；
* 	2014.05`  `“蓝桥杯”程序设计大赛国家二等奖、江苏省一等奖；
* 	2014.07`  `“中国计算机设计大赛”国家三等奖；
* 	2015.05`  `江苏省三好学生；
* 	2012-2015`  `南京信息工程大学校三好学生、校优秀共青团员、校优秀学干、校学生会优秀部长；

### 专利&论文
*  李剑；钱建军；杨健。一种鲁棒的工业电表数字识别方法。公开号: CN106778754A;
* 	梅园；李剑。一种基于改进的偏微分模型的指纹方向场计算方法。公开号：CN104680148A；

*    Jian Li, Jianjun Qian, Jian Yang. Object Detection via Feature Fusion based Single Network. 2017 ICIP.
*    Jian Li, Jianjun Qian, Yuhui Zheng. Ensemble R-FCN for Object Detection. 2017 FutureTech.
* 	Yunjie Chen, Jian Li, Hui Zhang, Yuhui Zheng, B. Jeon and Q. J. Wu. Non-local-based spatially constrained hierarchical fuzzy C-means method for brain magnetic resonance imaging segmentation, IET Image Processing, DOI: 10.1049/iet-ipr.2016.0271 , Online ISSN 1751-9667 Available online: 13 June 2016.

